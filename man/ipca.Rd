% Generated by hand for new iPCA API
\name{ipca}
\alias{ipca}
\alias{ipca.list}
\alias{ipca.multiblock}
\title{Integrated Principal Components Analysis (generic)}
\usage{
ipca(
  data,
  preproc,
  ncomp = 2,
  lambda = 1,
  method = c("auto", "gram", "dense"),
  max_iter = 100,
  tol = 1e-06,
  normalize_trace = TRUE,
  use_future = FALSE,
  eig_solver = c("auto", "full", "truncated"),
  eig_rank = NULL,
  eig_trunc_min_n = 400,
  ...
)

\method{ipca}{list}(
  data,
  preproc = multivarious::center(),
  ncomp = 2,
  lambda = 1,
  method = c("auto", "gram", "dense"),
  max_iter = 100,
  tol = 1e-06,
  normalize_trace = TRUE,
  use_future = FALSE,
  eig_solver = c("auto", "full", "truncated"),
  eig_rank = NULL,
  eig_trunc_min_n = 400,
  ...
)

\method{ipca}{multiblock}(
  data,
  preproc = multivarious::center(),
  ncomp = 2,
  lambda = 1,
  method = c("auto", "gram", "dense"),
  max_iter = 100,
  tol = 1e-06,
  normalize_trace = TRUE,
  use_future = FALSE,
  eig_solver = c("auto", "full", "truncated"),
  eig_rank = NULL,
  eig_trunc_min_n = 400,
  ...
)
}
\arguments{
\item{data}{A data object for which an iPCA method is defined. Typically a list
of matrices/data frames or a multiblock object.}

\item{preproc}{A preprocessing pipeline from the multivarious package. Each
block is preprocessed independently.}

\item{ncomp}{Integer; number of joint components to return.}

\item{lambda}{Positive numeric scalar (recycled) or vector of length equal to the
number of blocks. These are the multiplicative Frobenius penalties.}

\item{method}{One of \code{"auto"}, \code{"gram"}, or \code{"dense"}.}

\item{max_iter}{Integer; maximum Flip-Flop iterations.}

\item{tol}{Positive numeric convergence tolerance.}

\item{normalize_trace}{Logical; if \code{TRUE}, enforce \code{mean(diag(Sigma)) = 1}
after each iteration for numerical stability.}

\item{use_future}{Logical; if \code{TRUE}, block-wise updates are parallelized via
\code{furrr::future_map()} when available.}

\item{eig_solver}{Eigensolver policy for \code{method = "dense"}: \code{"full"},
\code{"truncated"}, or \code{"auto"}.}

\item{eig_rank}{Optional rank for truncated eigendecomposition in dense mode. Must
be at least \code{ncomp}.}

\item{eig_trunc_min_n}{Minimum sample size \code{n} at which \code{eig_solver =
"auto"} may switch to truncated eigendecomposition in dense mode.}

\item{...}{Additional arguments passed to the underlying method.}
}
\value{
An object inheriting from class \code{ipca}.
}
\description{
A generic front-end for integrated principal components analysis (iPCA) for
multiple aligned data blocks.
}
\details{
\code{ipca.list()} converts a list of blocks to a \code{multiblock} object and
dispatches to \code{ipca.multiblock()}.

\code{ipca.multiblock()} implements multiplicative Frobenius iPCA from Tang \&
Allen (2021) using a Flip-Flop algorithm. In \code{method = "gram"} mode, fitting
is performed in sample space (\eqn{n \times n}) and avoids \eqn{p_k \times p_k}
eigendecompositions.

\strong{When iPCA tends to shine}
\itemize{
  \item You want one shared sample-space representation across blocks, together with block-specific loadings.
  \item Blocks are high-dimensional (\eqn{p_k \gg n}) and memory-efficient sample-space fitting is important.
  \item Block covariance/noise structures differ and a joint matrix-normal model is desirable.
}

\strong{When alternatives may be preferable}
\itemize{
  \item If maximizing cross-block correlation is the primary goal, \code{mcca()} may perform better.
  \item If the setting closely matches MFA assumptions and runtime is the top priority, \code{mfa()} can be faster.
}

\strong{Practical defaults}
\itemize{
  \item Start with \code{method = "auto"}.
  \item Use \code{method = "gram"} whenever any block has \eqn{p_k > n}.
  \item Dense mode supports \code{eig_solver = "truncated"} (or \code{"auto"}) for large \eqn{n}; check stability against \code{"full"} if benchmarking sensitivity matters.
}
}
\examples{
\donttest{
set.seed(1)
X <- list(
  X1 = matrix(rnorm(60 * 40), 60, 40),
  X2 = matrix(rnorm(60 * 80), 60, 80),
  X3 = matrix(rnorm(60 * 50), 60, 50)
)
fit <- ipca(X, ncomp = 3, lambda = 1)
stopifnot(ncol(multivarious::scores(fit)) == 3)
}
}
\references{
Tang, T. M., \& Allen, G. I. (2021). Integrated Principal Components Analysis.
\emph{Journal of Machine Learning Research}, 22(198), 1-81.
}
