% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/penalized_mfa.R
\name{adam_update_block}
\alias{adam_update_block}
\title{Perform a single Adam update step on a block's loadings.}
\usage{
adam_update_block(
  V,
  G,
  M,
  V2,
  step_count,
  beta1,
  beta2,
  adam_epsilon,
  learning_rate
)
}
\arguments{
\item{V}{Current loading matrix (\eqn{p \times k}).}

\item{G}{The gradient of the loss with respect to V (\eqn{p \times k}).}

\item{M, V2}{Current first and second moment estimate matrices.}

\item{step_count}{The global step number, used for bias correction.}

\item{beta1, beta2}{Adam hyperparameters (e.g., 0.9, 0.999).}

\item{adam_epsilon}{A small constant to prevent division by zero (e.g., 1e-8).}

\item{learning_rate}{The base step size for the update.}
}
\value{
A list containing the updated `V`, `M`, and `V2` matrices.
}
\description{
This function updates the loadings matrix \code{V} using one step of the Adam
optimizer. It maintains and returns the updated first (\code{M}) and second
(\code{V2}) moment estimates. The update is performed in the ambient space.
}
\keyword{internal}
